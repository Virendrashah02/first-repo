{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU2Wf9tw33ta/+pCTvJ+Pd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Virendrashah02/first-repo/blob/main/data_prepration_phase_to_model_the_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "         @ 4. DATA PREPRATION PHASE TO MODEL THE DATA----------"
      ],
      "metadata": {
        "id": "E70wnY3EqDla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". 1  Partitioning the Data\n",
        "\n",
        "Partitioning data into training, validation, and test sets is crucial for building machine learning models. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set evaluates the model's performance."
      ],
      "metadata": {
        "id": "Ups0HVHlnKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': np.random.rand(100),  # Random numbers between 0 and 1\n",
        "    'Feature2': np.random.rand(100),\n",
        "    'Label': np.random.choice([0, 1], size=100)  # Binary classification labels\n",
        "})\n",
        "\n",
        "# Partition the data: 70% training, 15% validation, 15% test\n",
        "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training Data Size:\", train_data.shape)\n",
        "print(\"Validation Data Size:\", validation_data.shape)\n",
        "print(\"Test Data Size:\", test_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H-YlE5bnPzi",
        "outputId": "183c8e05-aac6-4efb-9d65-ea6a9fd35d94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size: (70, 3)\n",
            "Validation Data Size: (15, 3)\n",
            "Test Data Size: (15, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.train_test_split: Splits the data into training and temporary datasets. Then, the temporary data is further split into validation and test sets.\n",
        "\n",
        "2.Input Data: Feature1 and Feature2 are features, while Label is the target.\n",
        "Proportions:\n",
        "\n",
        "70% of data is used for training (train_data).\n",
        "The remaining 30% is split equally into validation (validation_data) and test sets (test_data).\n",
        "\n",
        "\n",
        "3.Output:\n",
        "Shapes of each dataset confirm the splits (e.g., train_data will have ~70 rows if the dataset has 100 rows)."
      ],
      "metadata": {
        "id": "LN7fChJLnX53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Balancing the Training Dataset\n",
        "\n",
        "Imbalanced data occurs when one class significantly outweighs the other,\n",
        "\n",
        "leading to biased models. Balancing ensures equal representation of classes, which can be achieved using oversampling (e.g., SMOTE), undersampling, or other techniques"
      ],
      "metadata": {
        "id": "4e6ebAgCnpF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "                           weights=[0.1, 0.9], n_informative=3, n_redundant=1,\n",
        "                           flip_y=0, n_features=5, n_clusters_per_class=1, n_samples=200, random_state=42)\n",
        "\n",
        "# Before balancing\n",
        "print(\"Before Balancing:\", pd.Series(y).value_counts())\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# After balancing\n",
        "print(\"After Balancing:\", pd.Series(y_resampled).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTR8I7gen1sL",
        "outputId": "474fd6a3-82f5-4fc0-9b8e-3bd8405ce318"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Balancing: 1    180\n",
            "0     20\n",
            "Name: count, dtype: int64\n",
            "After Balancing: 1    180\n",
            "0    180\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imbalanced Dataset:\n",
        "make_classification generates synthetic data with 90% of samples in one class (weights=[0.1, 0.9]).\n",
        "\n",
        "  pd.Series(y).value_counts() shows the original class distribution.\n",
        "\n",
        "2. SMOTE:\n",
        "SMOTE generates synthetic samples for the minority class to balance the dataset\n",
        ".\n",
        "fit_resample() returns a new dataset with balanced class representation.\n",
        "\n",
        "3. Output:\n",
        "Before Balancing: Shows the dataset is imbalanced (e.g., 180 instances of one class, 20 of another).\n",
        "\n",
        "4.  After Balancing: Confirms equal class distribution (e.g., 180:180)."
      ],
      "metadata": {
        "id": "u60iR9MFn6vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Building CART Decision Trees\n",
        "CART (Classification and Regression Trees) split the data into subsets using Gini impurity or Mean Squared Error for classification and regression tasks, respectively."
      ],
      "metadata": {
        "id": "pnz37KfaoY1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Build a CART decision tree\n",
        "cart_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "cart_model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "predictions = cart_model.predict(X)\n",
        "print(classification_report(y, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQo_mDQpofbe",
        "outputId": "724dda96-c33f-46b7-cb0d-825791a318d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       1.00      1.00      1.00        50\n",
            "           2       1.00      1.00      1.00        50\n",
            "\n",
            "    accuracy                           1.00       150\n",
            "   macro avg       1.00      1.00      1.00       150\n",
            "weighted avg       1.00      1.00      1.00       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset:\n",
        "load_iris() provides the Iris dataset with 4 features and 3 classes.\n",
        "X: Features, y: Labels.\n",
        "2. CART Model:\n",
        " DecisionTreeClassifier: Builds a classification tree.\n",
        "\n",
        "  criterion='gini': Uses Gini impurity to split nodes.\n",
        "4. Evaluation:\n",
        "classification_report: Evaluates precision, recall, and F1-score for each class.\n",
        "5. Output:\n",
        "Precision, recall, and F1-score indicate how well the tree classifies the data.\n",
        "\n",
        " Perfect scores (1.00) imply overfitting to the training data.\n"
      ],
      "metadata": {
        "id": "-7jOaGNmorX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "     ---------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "KElpMCNiphsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Building C5.0 Decision Trees\n",
        "C5.0 is an improvement over C4.5 but is not directly supported in Scikit-learn.\n",
        "\n",
        " Instead, we simulate it using entropy-based decision trees."
      ],
      "metadata": {
        "id": "veLkhrn5pLrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Generate a small dataset\n",
        "X = np.random.rand(100, 2)  # Two random features\n",
        "y = np.random.choice([0, 1], size=100)  # Binary target\n",
        "\n",
        "# Build a C5.0-like Decision Tree\n",
        "model = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X)\n",
        "print(\"Accuracy:\", accuracy_score(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9en3LiWpUcr",
        "outputId": "656e6bc1-ab34-4992-f455-968f3719d7c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Entropy:\n",
        "Unlike Gini, criterion='entropy' uses information gain for splits.\n",
        "2. Model Training:\n",
        "Trains on the generated dataset (X, y).\n",
        "3. Evaluation:\n",
        "accuracy_score: Measures the proportion of correct predictions.\n",
        "4. Output:\n",
        "Shows training accuracy. High accuracy may indicate overfitting.\n"
      ],
      "metadata": {
        "id": "1k1iRGN8prMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Summary\n",
        "\n",
        "* Partitioning: Splits data into training, validation, and test sets for unbiased evaluation.\n",
        "* Balancing: Ensures equal class representation, improving model fairness and accuracy.\n",
        "* CART Trees: Gini-based decision trees that split data for optimal classification or regression.\n",
        "* C5.0 Trees: Entropy-based trees with improved pruning mechanisms.\n",
        "* Random Forests: Ensemble method that reduces overfitting and improves accuracy."
      ],
      "metadata": {
        "id": "Is12kD3fp2Ft"
      }
    }
  ]
}