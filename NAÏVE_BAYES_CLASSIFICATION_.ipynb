{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5evcSMIGsw2oKhPYwLjN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Virendrashah02/first-repo/blob/main/NA%C3%8FVE_BAYES_CLASSIFICATION_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGV7KPQ725_d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naïve Bayes classifier is widely used in machine learning due to its simplicity, efficiency, and effectiveness for certain types of problems. Here's why it's used:\n",
        "\n",
        "### 1. **Simplicity and Speed**\n",
        "- **Easy to Implement**: The Naïve Bayes algorithm is straightforward to implement.\n",
        "- **Fast Computation**: It works efficiently, even on large datasets, because it makes strong assumptions about the independence of features (hence the term *naïve*).\n",
        "- **Low Computational Cost**: Training and prediction are computationally inexpensive, making it suitable for real-time applications.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Works Well for High-Dimensional Data**\n",
        "- In problems with many features (e.g., text classification where each word can be a feature), Naïve Bayes handles high-dimensional data efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Effective for Certain Types of Data**\n",
        "- **Categorical Data**: The Naïve Bayes classifier, particularly MultinomialNB or BernoulliNB, performs well for tasks involving categorical data, such as text classification.\n",
        "- **Continuous Data**: GaussianNB assumes a Gaussian distribution for continuous features, making it effective for datasets like the Iris dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Probabilistic Interpretation**\n",
        "- Naïve Bayes provides the **probability** of each class for a given input. This is useful for decision-making and understanding model confidence.\n",
        "  \\[\n",
        "  P(\\text{class}|\\text{features}) = \\frac{P(\\text{features}|\\text{class}) \\cdot P(\\text{class})}{P(\\text{features})}\n",
        "  \\]\n",
        "  This formula is based on **Bayes' Theorem**, which provides a solid probabilistic foundation.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Handles Missing Data**\n",
        "- Naïve Bayes can handle missing data by ignoring missing features during probability computation, which can simplify preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Performs Well with Small Data**\n",
        "- Despite its simplicity, Naïve Bayes often performs surprisingly well even when the dataset is small, especially in comparison to more complex algorithms that may overfit or require more data for good generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Applications**\n",
        "Naïve Bayes is used in various domains, including:\n",
        "- **Text Classification**: Spam detection, sentiment analysis, and document classification.\n",
        "- **Medical Diagnosis**: Predicting diseases based on symptoms.\n",
        "- **Customer Behavior Analysis**: Predicting customer churn or preferences.\n",
        "- **Recommendation Systems**: Suggesting products or content based on past behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Limitations**\n",
        "- **Independence Assumption**: It assumes all features are independent, which may not hold in real-world data. However, it often performs well even when this assumption is violated.\n",
        "- **Zero Probability Problem**: If a feature-class combination never appears in the training data, the probability estimate will be zero. This can be mitigated using techniques like **Laplace smoothing**.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "Naïve Bayes is a versatile and efficient classifier, especially suited for applications where interpretability, speed, and performance on small or high-dimensional datasets are crucial. It serves as a strong baseline model for many classification tasks."
      ],
      "metadata": {
        "id": "GXBbVsAW4MHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how you can demonstrate the application of the Naïve Bayes classifier in Python. We'll use the scikit-learn library, a popular machine learning library, and the Iris dataset, a classic dataset for classification problems.\n",
        "\n",
        "Step 1: Install Required Libraries\n",
        "First, ensure you have scikit-learn and pandas installed. You can install them using:"
      ],
      "metadata": {
        "id": "y6IbACKv4kb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW-_sRQY4pun",
        "outputId": "91671eb8-e202-4205-a21d-f003f29d3e31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Naïve Bayes Classification in Python\n",
        "Below is the code to demonstrate a Naïve Bayes classification:"
      ],
      "metadata": {
        "id": "gdDYnpNs41gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Naïve Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOfWYfaz42Z7",
        "outputId": "bb713fae-6c89-4d45-eb6d-b06c5fcf64ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.78%\n",
            "\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0 13]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Load the Dataset: The Iris dataset contains 3 classes of flowers: Setosa, Versicolour, and Virginica.\n",
        "Split Data: We split the data into training and testing sets (70% training, 30% testing).\n",
        "Train the Classifier: We use the GaussianNB model (appropriate for continuous data).\n",
        "Predict and Evaluate: Predictions are made on the test set, and we evaluate the model's accuracy and performance using a confusion matrix and classification report.\n",
        "Output:\n",
        "The script will output the following:\n",
        "\n",
        "Accuracy: The percentage of correct predictions.\n",
        "Confusion Matrix: Shows the performance of the classification.\n",
        "Classification Report: Contains precision, recall, and F1-score for each class.\n",
        "You can modify this example to use different datasets or Naïve Bayes variations like MultinomialNB or BernoulliNB depending on the problem type."
      ],
      "metadata": {
        "id": "UmCk3K3S5EZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through the code and explain the output in detail using the Iris dataset example.\n",
        "\n",
        "Step 1: Load the Iris Dataset\n",
        "The Iris dataset is a collection of measurements for 150 iris flowers from three different species:\n",
        "\n",
        "Setosa (label 0)\n",
        "Versicolour (label 1)\n",
        "Virginica (label 2)\n",
        "Each flower has four features:\n",
        "\n",
        "Sepal length\n",
        "Sepal width\n",
        "Petal length\n",
        "Petal width\n",
        "Step 2: Train-Test Split\n",
        "We split the dataset into training (70%) and testing (30%) sets to evaluate the model’s performance.\n",
        "\n",
        "Step 3: Training the Model\n",
        "The GaussianNB model assumes the features follow a Gaussian (normal) distribution and computes probabilities for classification.\n",
        "\n",
        "Step 4: Predict and Evaluate\n",
        "Here’s the output you might see after running the code:\n",
        "\n",
        "Accuracy\n",
        "plaintext\n",
        "Copy code\n",
        "Accuracy: 97.78%\n",
        "This means that 97.78% of the predictions made by the model are correct on the test dataset.\n",
        "\n",
        "Confusion Matrix\n",
        "plaintext\n",
        "Copy code\n",
        "Confusion Matrix:\n",
        " [[16  0  0]\n",
        "  [ 0 13  1]\n",
        "  [ 0  0 15]]\n",
        "Explanation:\n",
        "\n",
        "Rows represent the true labels.\n",
        "Columns represent the predicted labels.\n",
        "Each cell (i, j) indicates the number of samples with true label i and predicted label j:\n",
        "\n",
        "[16, 0, 0]: All 16 Setosa flowers were correctly classified.\n",
        "[0, 13, 1]: 13 Versicolour flowers were correctly classified, but 1 was misclassified as Virginica.\n",
        "[0, 0, 15]: All 15 Virginica flowers were correctly classified.\n"
      ],
      "metadata": {
        "id": "og2fzT9O-cCa"
      }
    }
  ]
}